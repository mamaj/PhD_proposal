\chapter{Entropy-based Lossy Compression} \label{chap-3}

\section{Problem Definition}

Let us consider the following general lossy compression setup:

\begin{equation}
    X \xrightarrow{\enc(T|X)} T \xrightarrow{\dec(Y|T)} Y
\end{equation}

where input $X$ with marginal distribution $P_X$ is encoded using the probabilistic encoder $\enc$ to generate code $T$. In turn, the probabilistic decoder $\dec$ reconstructs $Y$ from the code $T$. The goal is to find the encoder and decoder that minimize the expected code length $H(T)$, given a distortion constraint on $Y$. It is common to measure the sample-wise distortion via direct comparison of $(x, y)$ pairs through a distortion function $d(\cdot, \cdot)$, and consider the expectation as a notion of overall distortion, $\ex \left[ d(X,Y)  \right]$. Instead, we will consider log-loss $H(X|Y)$ as an alternative to enforce distortion constraint. Therefore, the optimization problem can be stated as follows:

\begin{equation}
\begin{aligned} \label{eq:no_const}
    \min_{\enc, \dec} \ &H(T)\\
    \textrm{s.t.} \ &H(X|Y) \leq \theta
\end{aligned}
\end{equation}

It is easy to see the optimal solution of (\ref{eq:no_const}) happens when $T=Y$, i.e. the identity decoder. To overcome this we constrain the output marginal distribution:

\begin{equation}
\begin{aligned} \label{eq:no_const}
    \min_{\enc, \dec} \ &H(T) \\
    \textrm{s.t.} \ &H(X|Y) \leq \theta \\
    &P(Y) = P_Y
\end{aligned}
\end{equation}

or equivalently:

\begin{equation} 
\begin{aligned} \label{eq:general}
    \min_{\enc, \dec} \ &H(T) - \beta I(X;Y) \\
    \textrm{s.t.} \ &P(Y) = P_Y
\end{aligned}
\end{equation}

A special case of (\ref{eq:general}) is when the code bottleneck is removed, i.e. $\beta \to \infty$. In this case, $X=T$ and the optimization becomes:

\begin{equation} 
\begin{aligned} \label{eq:maxmutual}
    \max_{\enc} \ &I(X;Y) \\
    \textrm{s.t.} \ &P(Y) = P_Y \\
\end{aligned}
\end{equation}

That is, finding the probabilistic pairing between the marginals $P_X$ and $P_Y$ in order to maximize the mutual information. We refer to (\ref{eq:maxmutual}) as the \textit{Maximum Information Matching} problem.


\section{Maximum Information Matching}

Consider two random variables $X$ and $Y$, over alphabets $\mathcal{X}$ and $\mathcal{Y}$ with probability mass functions $P_X$ and $P_Y$, respectively. The goal of Maximum Information Matching is to find the joint probability mass function $P_{XY}$ that maximizes the mutual information $I(X;Y)$, or equivalently minimizes the joint entropy $H(X;Y)$:

\begin{equation} 
\begin{aligned} \label{eq:minent}
    \min_{P_{XY}} \ &H(X;Y) \\
    \textrm{s.t.} \
        &\sum_{y\in\mathcal{Y}} P_{XY}(x, y) = P_X(x) \quad \forall x\in{\mathcal{X}},  \\
        &\sum_{x\in\mathcal{X}} P_{XY}(x, y) = P_Y(y) \quad \forall y\in{\mathcal{Y}}
\end{aligned}
\end{equation}

This is a concave minimization problem over a standard polyhedron. Therefore, every vertex of the polyhedron is a local minimum and the global minimum happens at a subset of the vertices.

Note that an standard polyhedron is defined as $\mathcal{P}=\{\bm{x} \in \reals^n | \ \bm{A}\bm{x} = \bm{b}, \ \bm{x} \geq \bm{0}\}$, where $\bm{A} \in \reals ^{m \times n}$ with linearly independent rows. A point $\bm{x}^* \in \mathcal{P}$ is a vertex if and only if it has $n-m$ zero elements and columns of $\bm{A}$ corresponding to other $m$ non-zero elements are linearly independent. Hence, to exhaustively iterate all the vertices:
\begin{enumerate}
    \item Choose $m$ linearly independent columns $\bm{A}_{\pi(1)}, \cdots, \bm{A}_{\pi(1)}$.
    \item Let $\bm{x}_i= 0$ for all  $i \in \pi(1),..., \pi(m)$
    \item Solve the system of $m$ equations $\bm{A}\bm{x} - \bm{b}$ for the unknowns $\bm{x}_\pi(1), \cdots, \bm{x}_\pi(m)$
\end{enumerate}

Therefore a crude upper-bound on the number of vertices would be ${n \choose m}$. This is enhanced by [] to ${n-m/2 \choose m/2}$ which is still exponential in $m$. 
Next, we will show that Maximum Information Matching problem as defined in (\ref{eq:minent}) is essentially NP-Hard. This is done by reduction from another NP-complete problem, $k$-Subset-Sum.

\begin{remark} Maximum Information Matching problem of (\ref{eq:minent}) is NP-Hard.
\end{remark}
\begin{proof}
To show an optimization problem is NP-Hard, we need to show the corresponding decision problem is NP-Hard.  Given an optimization problem, a decision version is whether or not any target value $T$ is achievable. Without the loss of generality, assume $\mathcal{X} > \mathcal{Y}$. We set $T=H(Y)$, i.e. to decide if there exists a function $f: \mathcal{X}\to\mathcal{Y}$ such that $Y=f(X)$. Let's call this problem \textit{Deterministic Matching}.

Next, we show any instance of the $k$-Subset-Sum problem can be reduced to an instance of Deterministic Matching, by a polynomial-time procedure. Consider a general instance of the $k$-Subset-Sum problem: Given set $\mathcal{S}$ of integers and target values $\{t_i|1\leq i\leq k\}$, decide if there exists a partition $\{\mathcal{S}_i|1\leq i\leq k \}$ of size $k$ on $\mathcal{S}$ such that $\sum \mathcal{S}_i = t_i$ for all $1\leq i\leq k$. 
Now, set $P_X(i)=s_i/\sum(\mathcal{S}), \forall s_i \in \mathcal{S}$ and $P_Y(i)=t_i / \sum(t_j)$. Then, clearly solving Deterministic Matching for $P_X, P_Y$ will solve the original $k$-Subset-Sum problem. Therefore, $k$-Subset-Sum  $<_p$ Deterministic Matching and hence, Deterministic Matching is NP-Hard. Consequently, Maximum Information Matching is a NP-Hard optimization problem.
\end{proof}

Finally, we introduce two linear-time approximate greedy algorithms for Maximum Information Matching problem, and numerically compare their achieved minima to a general approximate algorithm.

\RestyleAlgo{ruled}
\begin{algorithm}[H]
$P_{XY}(x, y) \gets 0, \quad \forall x, y \in \mathcal{X}, \mathcal{Y}$ \; 
\While{marginals $P_X, P_Y \neq \bm{0}$}{
  $x^* \gets \argmax_{x} P_X(x)$ \;
  $y^* \gets \argmax_{y} P_Y(y)$ \;
  $P_{XY}(x^*, y^*) = \min\{P_X(x^*), P_Y(y^*)\}$ \;
  $P_X(x^*) \gets  P_X(x^*) - \min\{P_X(x^*), P_Y(y^*)\}$ \;
  $P_Y(y^*) \gets  P_Y(y^*) - \min\{P_X(x^*), P_Y(y^*)\}$ \;
 }
 \caption{Max-Seeking Greedy}
\end{algorithm}

\RestyleAlgo{ruled}
\begin{algorithm}[H]
$P_{XY}(x, y) \gets 0, \quad \forall x, y \in \mathcal{X}, \mathcal{Y}$ \; 
\While{marginals $P_X, P_Y \neq \bm{0}$}{
  $(x^*, y^*) \gets \argmin_{x, y}|P_X(x)-P_Y(y)|$ \;
%   (break ties by picking the largest marginal) \\
  $P_{XY}(x^*, y^*) = \min\{P_X(x^*), P_Y(y^*)\}$ \;
  $P_X(x^*) \gets  P_X(x^*) - \min\{P_X(x^*), P_Y(y^*)\}$ \;
  $P_Y(y^*) \gets  P_Y(y^*) - \min\{P_X(x^*), P_Y(y^*)\}$ \;
}
\caption{Zero-Seeking Greedy}
\end{algorithm}

As a simple baseline, we randomly generated 100 pairs of joint distributions and fed them to our greedy solvers. We also used a general concave minimization method, Successive Linearization Algorithm (SLA), and compared the achieved joint entropy. Table \ref{table:maxIresult} summarizes the average joint entropy over 100 trials for each method.

\begin{table}[h]
\center
\caption{Maximum Information Matching: average achieved joint entropy of 100 simulations of marginal distributions.}
\begin{tabular}{ |c|c| }
 \hline
 Name & Entropy  \\ 
 \hline
 Independent Joint & $5.443 \pm 0.101$  \\ 
 SLA & $3.225 \pm 0.141$\\ 
 Max-Seeking Greedy & $2.946 \pm 0.064$\\
 Zero-Seeking Greedy & $2.937 \pm 0.058$\\
 \hline
\end{tabular}
\label{table:maxIresult}
\end{table}


